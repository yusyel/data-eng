{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/09 13:42:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark context Web UI available at http://de-zoomcamp.europe-west1-b.c.pelagic-logic-375820.internal:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1678369365193).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.2)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "!spark-shell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 3.3.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/11 23:21:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('homework') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02764|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:27:01|2021-06-01 00:42:11|         240|         127|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:46:08|2021-06-01 00:53:45|         127|         235|      N|                B02764|\n",
      "|              B02510|2021-06-01 00:45:42|2021-06-01 01:03:33|         144|         146|      N|                  null|\n",
      "|              B02510|2021-06-01 00:18:15|2021-06-01 00:25:47|          49|          17|      N|                  null|\n",
      "|              B02510|2021-06-01 00:33:06|2021-06-01 00:42:46|          49|         225|      N|                  null|\n",
      "|              B02510|2021-06-01 00:46:27|2021-06-01 00:56:50|         225|         177|      N|                  null|\n",
      "|              B02764|2021-06-01 00:48:06|2021-06-01 01:04:10|         209|          45|      N|                B02764|\n",
      "|              B02875|2021-06-01 00:18:54|2021-06-01 00:26:14|          80|         256|      N|                B02875|\n",
      "|              B02875|2021-06-01 00:31:02|2021-06-01 00:36:39|         217|          17|      N|                B02875|\n",
      "|              B02875|2021-06-01 00:41:53|2021-06-01 01:07:32|          17|         265|      N|                B02875|\n",
      "|              B02875|2021-06-01 00:29:52|2021-06-01 00:54:41|         210|          76|      N|                B02875|\n",
      "|              B02510|2021-06-01 00:15:57|2021-06-01 00:39:36|         226|         213|      N|                  null|\n",
      "|              B02510|2021-06-01 00:11:59|2021-06-01 00:23:32|         191|           9|      N|                  null|\n",
      "|              B02510|2021-06-01 00:30:35|2021-06-01 00:45:35|          16|         250|      N|                  null|\n",
      "|              B02510|2021-06-01 00:49:01|2021-06-01 01:03:50|         182|         259|      N|                  null|\n",
      "|              B02510|2021-06-01 00:07:36|2021-06-01 00:21:13|         188|          72|      N|                  null|\n",
      "|              B02510|2021-06-01 00:25:48|2021-06-01 00:40:43|          39|          72|      N|                  null|\n",
      "|              B02510|2021-06-01 00:46:11|2021-06-01 00:53:39|          72|          35|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "types.StructField('PULocationID', types.IntegerType(), True),\n",
    "types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "types.StructField('SR_Flag', types.StringType(), True),\n",
    "types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(12)\n",
    "df.write.parquet('fhvhv/2021/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24M\t./fhvhv/2021/part-00000-fbfce35b-8c7e-4332-816d-3c5be06bcf3a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./fhvhv/2021/part-00000-fbfce35b-8c7e-4332-816d-3c5be06bcf3a-c000.snappy.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "**24m**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./fhvhv/2021/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02889|2021-06-04 20:51:44|2021-06-04 21:10:12|         239|         158|      N|                B02889|\n",
      "|              B02800|2021-06-04 15:50:15|2021-06-04 16:19:29|          75|         116|      N|                  null|\n",
      "|              B02510|2021-06-02 21:03:38|2021-06-02 21:10:12|         167|         168|      N|                  null|\n",
      "|              B02867|2021-06-02 12:51:57|2021-06-02 13:05:09|         151|         142|      N|                B02867|\n",
      "|              B02869|2021-06-21 09:51:45|2021-06-21 10:09:17|         106|          65|      N|                B02869|\n",
      "|              B02764|2021-06-02 13:27:03|2021-06-02 13:38:20|         113|         148|      N|                B02764|\n",
      "|              B02764|2021-06-10 14:48:23|2021-06-10 16:06:10|         250|         239|      N|                B02764|\n",
      "|              B02510|2021-06-09 22:38:49|2021-06-09 23:11:58|         132|          33|      N|                  null|\n",
      "|              B02510|2021-06-26 06:50:43|2021-06-26 07:00:20|         238|         244|      N|                  null|\n",
      "|              B02872|2021-06-07 08:04:00|2021-06-07 08:40:25|         198|         234|      N|                B02872|\n",
      "|              B02765|2021-06-24 20:19:42|2021-06-24 20:38:22|         147|          42|      N|                B02765|\n",
      "|              B02884|2021-06-05 17:38:24|2021-06-05 18:05:30|         142|         232|      N|                B02884|\n",
      "|              B02765|2021-06-13 15:44:27|2021-06-13 16:04:38|          26|         165|      N|                B02765|\n",
      "|              B02510|2021-06-26 18:37:10|2021-06-26 18:56:01|         237|         145|      N|                  null|\n",
      "|              B02764|2021-06-23 14:44:10|2021-06-23 14:58:06|          45|          33|      N|                B02764|\n",
      "|              B02510|2021-06-20 02:11:32|2021-06-20 02:22:00|          49|          37|      N|                  null|\n",
      "|              B02764|2021-06-29 19:02:50|2021-06-29 19:05:25|         140|         140|      N|                B02764|\n",
      "|              B02875|2021-06-02 15:27:05|2021-06-02 15:37:27|          32|         185|      N|                B02875|\n",
      "|              B02882|2021-06-18 12:48:43|2021-06-18 12:58:22|          95|         134|      N|                B02882|\n",
      "|              B02510|2021-06-23 08:59:54|2021-06-23 09:44:49|         171|         140|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yusuf/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(pickup_datetime)|\n",
      "+----------------------+\n",
      "|                452470|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(pickup_datetime) FROM df \\\n",
    "WHERE pickup_datetime BETWEEN '2021-06-15 00:00:00' AND '2021-06-15 23:59:59' \n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**452470**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "| max(diff_hours)|\n",
      "+----------------+\n",
      "|66.8788888888889|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df.select(max(datediff(df.dropoff_datetime, df.pickup_datetime).alias('diff'))).show()\n",
    "diff = col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")\n",
    "df2 = df.withColumn( \"diff_hours\", diff/ 3600 )\n",
    "df2.select(max(df2.diff_hours)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**66.87**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|hour|\n",
      "+----+\n",
      "|  66|\n",
      "|  25|\n",
      "|  19|\n",
      "|  18|\n",
      "|  16|\n",
      "|  14|\n",
      "|  13|\n",
      "|  11|\n",
      "|  11|\n",
      "|  10|\n",
      "|  10|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "|   9|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT DATEDIFF(hour, pickup_datetime, dropoff_datetime) as hour FROM df\n",
    "ORDER BY hour DESC;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***port 4040**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|          61|231279|\n",
      "|          79|221244|\n",
      "|         132|188867|\n",
      "|          37|187929|\n",
      "|          76|186780|\n",
      "|         231|164344|\n",
      "|         138|161596|\n",
      "|         234|158937|\n",
      "|         249|154698|\n",
      "|           7|152493|\n",
      "|         148|151020|\n",
      "|          68|147673|\n",
      "|          42|146402|\n",
      "|         255|143683|\n",
      "|         181|143594|\n",
      "|         225|141427|\n",
      "|          48|139611|\n",
      "|         246|139431|\n",
      "|          17|138428|\n",
      "|         170|137879|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.groupBy('PULocationID').count()\n",
    "df2.sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.join(zone, df.PULocationID == zone.LocationID, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------+\n",
      "|                Zone|PULocationID| count|\n",
      "+--------------------+------------+------+\n",
      "| Crown Heights North|          61|231279|\n",
      "|        East Village|          79|221244|\n",
      "|         JFK Airport|         132|188867|\n",
      "|      Bushwick South|          37|187929|\n",
      "|       East New York|          76|186780|\n",
      "|TriBeCa/Civic Center|         231|164344|\n",
      "|   LaGuardia Airport|         138|161596|\n",
      "|            Union Sq|         234|158937|\n",
      "|        West Village|         249|154698|\n",
      "|             Astoria|           7|152493|\n",
      "|     Lower East Side|         148|151020|\n",
      "|        East Chelsea|          68|147673|\n",
      "|Central Harlem North|          42|146402|\n",
      "|Williamsburg (Nor...|         255|143683|\n",
      "|          Park Slope|         181|143594|\n",
      "|  Stuyvesant Heights|         225|141427|\n",
      "|        Clinton East|          48|139611|\n",
      "|West Chelsea/Huds...|         246|139431|\n",
      "|             Bedford|          17|138428|\n",
      "|         Murray Hill|         170|137879|\n",
      "+--------------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Zone', 'PULocationID').count().sort(desc('count')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crown Heights North**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Özgürlük Caddesi\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "Latitude = \"41.0092163085930\"\n",
    "Longitude = \"29.1522216796870\"\n",
    "location = geolocator.reverse(Latitude+\",\"+Longitude)\n",
    "address = location.raw['address']\n",
    "print(address['road']) # Road/Route Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng-_qt8xORX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
